{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tirto.id\n",
    "\n",
    "Pada python notebook ini akan dijelaskan mengenai scraping artikel pada [tirto.id](https://tirto.id/)\n",
    "\n",
    "\n",
    "### Initialize Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from openpyxl import Workbook\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://tirto.id/indeks/1\")\n",
    "html_page = page.content\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "links = soup.find_all('script')\n",
    "\n",
    "#article is located on the script link [4]\n",
    "content_script = links[4].text.replace('window.__NUXT__=','')[:-1]\n",
    "article = json.loads(content_script)\n",
    "#print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in article['data'][0]['listarticle']:\n",
    "    ws.append([a['judul'], a['date_news'], \"https://tirto.id\"+a['articleUrl']])\n",
    "time.sleep(2)\n",
    "\n",
    "wb.save(\"scrap-data/tirto.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Scrapping Function\n",
    "Creating scrapping function from the test scrapping above. Combined all of the function and run automatically to scrap all data from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_link(i):\n",
    "    print(\"Crawling Page \"+str(i))\n",
    "    \n",
    "    page = requests.get(\"https://tirto.id/indeks/\"+str(i))\n",
    "    html_page = page.content\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    links = soup.find_all('script')\n",
    "    \n",
    "    isi_script = links[4].text.replace('window.__NUXT__=','')[:-1]\n",
    "    article = json.loads(isi_script)\n",
    "    \n",
    "    for a in article['data'][0]['listarticle']:\n",
    "        ws.append([a['judul'], a['date_news'], \"https://tirto.id\"+a['articleUrl']])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Page 1\n",
      "Crawling Page 2\n",
      "Crawling Page 3\n",
      "Crawling Page 4\n",
      "Crawling Page 5\n",
      "Crawling Page 6\n",
      "Crawling Page 7\n",
      "Crawling Page 8\n",
      "Crawling Page 9\n",
      "Crawling Page 10\n",
      "Crawling Page 11\n",
      "Crawling Page 12\n",
      "Crawling Page 13\n",
      "Crawling Page 14\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,15):\n",
    "    crawl_link(i)\n",
    "    \n",
    "wb.save(\"scrap-data/tirtoArticles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
